# Importación de librerías y cargar base de datos
"""

# Commented out IPython magic to ensure Python compatibility.
#Preparamos complementos, librerías y bases disponibles
import warnings

warnings.filterwarnings("ignore")

#Instalación de complementos
!pip install kneed
!pip install sweetviz
!pip install factor_analyzer
!pip install pandas numpy sweetviz seaborn matplotlib scikit-learn

# %matplotlib inline

#importar librerías
import numpy as np
import kneed as kd
import pandas as pd
import seaborn as sns
import sweetviz as sv
import plotly.express as px
import matplotlib.pyplot as plt

from sklearn import tree
from sklearn import metrics
from sklearn import datasets
from kneed import KneeLocator
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.compose import ColumnTransformer
from sklearn.mixture import GaussianMixture
from factor_analyzer import FactorAnalyzer
from sklearn.datasets import make_blobs
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from scipy.spatial.distance import cdist
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import ShuffleSplit
from sklearn.feature_selection import RFE

#Cargamos la base alojada en GitHub
BDD = pd.read_csv("https://raw.githubusercontent.com/veronica1908/T2_APRENDIZAJE_NO_SUPERVISADO/main/BASE_SIN_PROCESAR.csv", sep = ',')

#Verificamos la visualización
BDD.head()

"""# Preprocesamiento de los datos

##Información y estado de la base de datos
"""

#Verificamos las dimensiones de la base
BDD.shape

# Vemos información de las variables, como datos NO nulos y tipo de dato
BDD.info()

# Empezamos eliminando la variable N° que funciona como un identificador de losd atos de la base y que no aporta para el ejercicio.
BDD.drop(['N°',], axis=1, inplace=True)

# Vemos la cantidad de datos nulos por variable
BDD.isnull().sum()

#Existen muchos datos nulos y como las dimensiones de la base son considerablemente grandes, se eliminarán las filas cuyos datos nulos pertenezcan a variables núméricas importantes y para las filas de texto, se agregará "Sin información"

"""##Ajuste de las variables de la base de datos"""

#Vamos a verificar cada columna para tomar decisiones en el ajuste de datos de cada una de ellas
#Como la columna "Departamento defunción" no tiene datos nulos, verificamos entonces sus categorías en orden alfabético para detectar posibles errores en el ingreso de datos
sorted(BDD['Departamento defunción'].unique())
#Verificamos la cantidad de datos por departamento
BDD['Departamento defunción'].value_counts()

#Como la gran mayoría de datos pertenecen al departamento del VALLE DEL CAUCA, nos centraremos en estos datos y eliminaremos los que pertenecen a otros departamentos
dep_elim = ['CUNDINAMARCA','SANTA FE DE BOGOTÁ','RISARALDA','TOLIMA','NARIÑO','CASANARE','GUAVIARE','QUINDIO']
BDD = BDD[~BDD['Departamento defunción'].isin(dep_elim)]

#Como la columna "Municipio defunción" tampoco tiene datos nulos, verificamos también sus categorías
sorted(BDD['Municipio defunción'].unique())
#se observa que pueden combinarse las categorías "CALIMA' y 'CALIMA (DARIEN)'

#Se combinan las categorías "CALIMA' y 'CALIMA (DARIEN)' y BUGA con GUADALAJARA DE BUGA
BDD['Municipio defunción'] = BDD['Municipio defunción'].replace({'CALIMA (DARIEN)': 'CALIMA'})
BDD['Municipio defunción'] = BDD['Municipio defunción'].replace({'BUGA': 'GUADALAJARA DE BUGA'})
#Verificamos
sorted(BDD['Municipio defunción'].unique())

#Columna "Área defunción" tiene 3 datos nulos, los llenamos con el texto "SIN INFORMACIÓN" y verificamos también sus categorías
#Agregamos información a celdas vacías
BDD['Área defunción'].fillna('SIN INFORMACIÓN', inplace=True)
#Verificamos categorías
sorted(BDD['Área defunción'].unique())

#Unificamos las categorías: "CENTRO POBLADO (Inspección,\r\ncorregimiento o caserío)" y 'CENTRO POBLADO (INSPECCIÓN, CORREGIMIENTO O CASERÍO)' como "CENTRO POBLADO"
BDD['Área defunción'] = BDD['Área defunción'].replace({'"CENTRO POBLADO (Inspección,\r\ncorregimiento o caserío)"': 'CENTRO POBLADO', 'CENTRO POBLADO (INSPECCIÓN, CORREGIMIENTO O CASERÍO)':'CENTRO POBLADO'})
#Verificamos ajustes
sorted(BDD['Área defunción'].unique())

#Columna "Sitio defunción" no tiene datos nulos, verificamos categorías
sorted(BDD['Sitio defunción'].unique())
#No hay ajustes para esta variable

#Columna "Sitio defunción" no tiene datos nulos, verificamos categorías
sorted(BDD['Sitio defunción'].unique())
#No hay ajustes para esta variable

#Columna "Otro sitio de defunción" tiene 5254 nulos. Esta columna existe solo como complemento para la categoría "OTRO" de la variable "Sitio defunción",
#Por lo tanto, se reemplazará los datos nulos con el texto "NA" (no aplica), para las filas del resto de las categorías diferentes a "OTRO" que aparecen como nulos

#Agregamos información en celdas vacías
BDD['Otro sitio de defunción'].fillna('NA', inplace=True)

#Verificamos categorías
sorted(BDD['Otro sitio de defunción'].unique())

#Ponemos todas las categorías en mayúsculas, ya que solo "domicilio" está en minúscula.
BDD['Otro sitio de defunción'] = BDD['Otro sitio de defunción'].str.upper()
#Unificamos las categorías 'PRISIÓN REFORMATORIOS'y 'PRISIÓNREFORMATORIOS'
BDD['Otro sitio de defunción'] = BDD['Otro sitio de defunción'].replace({'PRISIÓNREFORMATORIOS': 'PRISIÓN REFORMATORIOS'})

#Verificamos nuevamente categorías
sorted(BDD['Otro sitio de defunción'].unique())

#Verificamos datos nulos
BDD['Otro sitio de defunción'].isnull().sum()

#Verificamos categorías de 'Nombre institución' de defunción que no tiene datos nulos
sorted(BDD['Nombre institución de defunción'].unique())
#Sin novedades en esta columna

#Verificamos categorías de 'Tipo de defunción' de defunción que no tiene datos nulos
sorted(BDD['Tipo de defunción'].unique())
#Sin novedades en esta columna

#La variable 'Fecha de Defunción' no tiene datos nulos, pero se ajusta el formato
# Convertir a formato de fecha
BDD['Fecha de Defunción'] = pd.to_datetime(BDD['Fecha de Defunción'])
#Verificamos
BDD.info('Fecha de Defunción')

#Como la columna "Hora de defunción" tiene 1072 datos vacíos, pero la columna anterior de fecha, contiene también la hora y no tenía datos nulos, se elimina la columna de hora para no considerarla.
BDD = BDD.drop('Hora de defunción', axis=1)

#Verificamos categorías de la columna 'sexo' que no tiene datos nulos.
sorted(BDD['Sexo'].unique())
#Sin novedades en esta columna

#Agregamos la información a los datos vacíos de la columna "Estado civil" que tiene 51 datos nulos, con la anotación "SIN INFORMACIÓN"
BDD['Estado civil'].fillna('SIN INFORMACIÓN', inplace=True)

#Verificamos categorías de la columna 'Estado civil'
sorted(BDD['Estado civil'].unique())

#Vamos a unificar algunas categorías
BDD['Estado civil'] = BDD['Estado civil'].replace({'ESTABA CASADO(A)': 'CASADO(A)',
                                                   'NO CASADO(A) Y DOS AÑOS O MAS CON SU PAREJA':'UNIÓN LIBRE',
                                                   'NO CASADO(A) Y MENOS DE DOS AÑOS CON SU PAREJA':'UNIÓN LIBRE',
                                                   'NO ESTABA CASADO(A) Y LLEVABA DOS AÑOS O MÁS VIVIENDO CON SU PAREJA':'UNIÓN LIBRE',
                                                   'NO ESTABA CASADO(A) Y LLEVABA MENOS DE DOS AÑOS VIVIENDO CON SU PAREJA':'UNIÓN LIBRE',
                                                   'ESTABA SEPARADO(A), DIVORCIADO(A)':'SEPARADO (A)',
                                                   'SEPARADO(A), DIVORCIADO(A)':'SEPARADO (A)',
                                                   'ESTABA SOLTERO(A)':'SOLTERO(A)',
                                                   'ESTABA VIUDO(A)':'VIUDO(A)'})

#Verificamos ajustes
sorted(BDD['Estado civil'].unique())

#Como la edad es una variable que podría tener gran significancia en esta base de datos, tener datos vacíos no sería útil y llenarlos con la media podría sesgar la información.
#Considerando que el conjunto de datos es de buenas dimensiones, se decide eliminar las filas con valores nulos en esta variable.
BDD = BDD.dropna(subset=['Edad'])

#Verificamos datos nulos
BDD['Edad'].isnull().sum()

#Ahora verificamos que los datos sean coherentes en cuanto a la edad
sorted(BDD['Edad'].unique())

#Como hay valores demasiado altos y esta podría ser una variable significativa, se eliminan las filas cuyos valores son mayores a 110 años 8 a partir de esa edad se observa un salto enorme que puede ser error de digitación)
#Convertimos la columna a tipo enteros
BDD['Edad'] = BDD['Edad'].astype(int)

#Eliminamos los valores mayores a 110
BDD = BDD[BDD['Edad'] <= 110]

#Verificamos nuevamente
sorted(BDD['Edad'].unique())

#Agregamos la información a los datos vacíos de la columna "Nivel educativo " que tiene 24 datos nulos, con la anotación "SIN INFORMACIÓN"
BDD['Nivel educativo'].fillna('SIN INFORMACIÓN', inplace=True)

#Verificamos categorías de la columna 'Nivel educativo '
sorted(BDD['Nivel educativo'].unique())

#Vamos a unificar algunas categorías
BDD['Nivel educativo'] = BDD['Nivel educativo'].replace({'BASICA PRIMARIA': 'BÁSICA PRIMARIA',
                                                   'BASICA SECUNDARIA':'BÁSICA SECUNDARIA',
                                                   'MAESTRIA':'MAESTRÍA',
                                                   'MEDIA ACADEMICA O CLASICA':'MEDIA ACADÉMICA',
                                                   'MEDIA ACADÉMICA O CLÁSICA':'MEDIA ACADÉMICA',
                                                   'MEDIA TECNICA':'MEDIA TÉCNICA',
                                                   'TECNICA PROFESIONAL':'TÉCNICA PROFESIONAL',
                                                   'TECNOLOGICA':'TECNOLÓGICA',
                                                   'NORMALISTA':'MEDIA TÉCNICA'})

#Verificamos ajustes
sorted(BDD['Nivel educativo'].unique())

#Como la columna "Último curso fallecido" tiene 1141 datos vacíos, pero la columna anterior de "Nivel educativo", contiene información similar, se elimina esta columna de la base
BDD = BDD.drop('Último curso fallecido', axis=1)

"""##Variable OCUPACIÓN"""

#Agregamos la información a los datos vacíos de la columna "Ocupación" que tiene 205 datos nulos, con la anotación "SIN INFORMACIÓN"
BDD['Ocupación'].fillna('SIN INFORMACIÓN', inplace=True)

#Verificamos categorías de la columna 'Ocupación'
sorted(BDD['Ocupación'].unique())

#Como son tantas categorías en esta variable, vamos a clasificar las categorías de "Ocupación" en una nueva columna de nombre "OCUPACION", tomando la parte general del oficio
#Renombramos la columna general
BDD.columns = BDD.columns.str.replace('Ocupación', 'Ocupación específica')

#Creamos la nueva columna
BDD['OCUPACION'] = BDD['Ocupación específica']

#Clasificamos las categorías agrupándolas así:

A= ['AGENTES COMERCIALES Y CORREDORES, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'AGENTES DE COMPRAS, INTERMEDIARIOS Y CONSIGNATARIOS',
 'AGENTES DE LA ADMINISTRACIÓN PÚBLICA EN ADUANAS, IMPUESTOS Y AFINES, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'AGENTES DE LA POLICIA NACIONAL',
 'AGENTES Y POLICIAS DE TRANSITO']

B= 'AGENTES'

C= ['TÉCNICOS EN AGRONOMÍA, ZOOTECNIA Y SILVICULTURA',
 'TÉCNICOS EN DISEÑO Y DECORADORES',
 'TÉCNICOS TERAPEUTAS, QUIROPRÁCTICOS Y AFINES',
 'TÉCNICOS Y ASISTENTES EN FARMACIA',
 'TÉCNICOS Y ASISTENTES VETERINARIOS',
 'TÉCNICOS Y POSTSECUNDARIOS NO UNIVERSITARIOS EN CIENCIAS FÍSICAS, QUÍMICAS E INGENIERÍAS, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'TÉCNICOS, POSTSECUNDARIOS NO UNIVERSITARIOS EN INGENIERÍA CIVIL, ARQUITECTURA, AGRIMENSORES Y AFINES',
 'TÉCNICOS, POSTSECUNDARIOS NO UNIVERSITARIOS Y ASISTENTES DE SERVICIOS ADMINISTRATIVOS Y AFINES',
 'TÉCNICOS, POSTSECUNDARIOS NO UNIVERSITARIOS Y ASISTENTES EN OPERACIONES COMERCIALES, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'T╔CNICOS E HIGIENISTAS DENTALES',
 'T╔CNICOS EN AGRONOM═A, ZOOTECNIA Y SILVICULTURA',
 'T╔CNICOS Y ASISTENTES EN FARMACIA']

D= 'TÉCNICOS'

E= ['OPERADORES DE EQUIPOS ÓPTICOS Y ELECTRÓNICOS, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'OPERADORES DE INSTALACIONES DE TRATAMIENTO DE AGUA Y AFINES',
 'OPERADORES DE MEZCLADORAS Y DE HORNOS DE VIDRIER═A Y AFINES',
 'OPERADORES DE MÁQUINAS PARA PROCESAR CARNE, PESCADO Y MARISCOS',
 'OPERADORES DE TELARES Y OTRAS MÁQUINAS TEJEDORAS',
 'OPERARIOS DE LA CONSERVACIÓN DE FRUTAS, LEGUMBRES, VERDURAS Y AFINES',
 'OPERARIOS DE LA FOTOGRAFÍA Y AFINES',
 'OPERARIOS DEL TRATAMIENTO DE LA MADERA',
 'OPERARIOS EN CEMENTO ARMADO, ENFOSCADORES Y AFINES',
 'OBREROS DE CARGA',
 'OBREROS DE LA CONSTRUCCIÓN DE EDIFICIOS',
 'OBREROS DE PESCA, CAZA Y TRAMPA',
 'OBREROS FORESTALES',
 'OBREROS Y PEONES AGROPECUARIOS DE LABRANZA Y DE INVERNADERO']

F='OPERARIOS'

G=['PROFESORES DE EDUCACIËN PRIMARIA',
 'PROFESORES DE EDUCACIËN SECUNDARIA',
 'PROFESORES DE EDUCACIÓN PRIMARIA',
 'PROFESORES DE EDUCACIÓN SECUNDARIA',
 'PROFESORES DE UNIVERSIDADES Y OTROS ESTABLECIMIENTOS DE EDUCACIËN SUPERIOR',
 'PROFESORES DE UNIVERSIDADES Y OTROS ESTABLECIMIENTOS DE EDUCACIÓN SUPERIOR',
 'PROFESORES E INSTRUCTORES DE EDUCACIËN ESPECIAL',
 'PROFESORES E INSTRUCTORES DE EDUCACIÓN ESPECIAL', 'PROFESIONALES DE LA EDUCACIËN, NO CLASIFICADOS BAJO OTROS EP═GRAFES',
 'PROFESIONALES DE LA EDUCACIÓN, NO CLASIFICADOS BAJO OTROS EPÍGRAFES']

H= 'PROFESORES'

I= [ 'VENDEDORES A DOMICILIO Y POR TELÉFONO',
 'VENDEDORES AMBULANTES',
 'VENDEDORES EN QUIOSCOS Y PUESTOS DE MERCADO',
 'VENDEDORES, DEMOSTRADORES DE TIENDAS Y ALMACENES']

J= 'VENDEDORES'

K=[ 'DIRECTORES DE DEPARTAMENTOS DE PRODUCCIËN Y OPERACIONES EN COMERCIO MAYORISTA Y MINORISTAS',
 'DIRECTORES DE DEPARTAMENTOS DE PRODUCCIÓN Y OPERACIONES EN AGRICULTURA, CAZA, SILVICULTURA Y PESCA',
 'DIRECTORES DE DEPARTAMENTOS DE PRODUCCIÓN Y OPERACIONES EN COMERCIO MAYORISTA Y MINORISTAS',
 'DIRECTORES DE DEPARTAMENTOS DE PRODUCCIÓN Y OPERACIONES EN CONSTRUCCIÓN Y OBRAS PÚBLICAS',
 'DIRECTORES DE DEPARTAMENTOS DE PRODUCCIÓN Y OPERACIONES EN INDUSTRIAS MANUFACTURERAS Y EXTRACTIVAS',
 'DIRECTORES GENERALES, DE EMPRESAS O ENTIDADES DE LA ADMINISTRACIËN P┌BLICA',
 'DIRECTORES Y GERENTES GENERALES DE EMPRESAS PRIVADAS',
 'DIRIGENTES Y ADMINISTRADORES DE ORGANIZACIONES DE EMPLEADORES, DE TRABAJADORES Y DE OTRAS DE INTERÉS SOCIOECONÓMICO']

L= 'DIRECTORES'

M= [ 'MECÁNICOS Y AJUSTADORES DE MÁQUINAS AGRÍCOLAS E INDUSTRIALES',
 'MECÁNICOS Y AJUSTADORES DE VEHÍCULOS DE MOTOR',
 'MECÁNICOS Y REPARADORES DE INSTRUMENTOS DE PRECISIÓN',
 'MEC┴NICOS Y AJUSTADORES DE VEH═CULOS DE MOTOR']

N= 'MECÁNICOS'

O= ['INGENIEROS CIVILES, INGENIEROS DE TRANSPORTE Y AFINES',
 'INGENIEROS ELÉCTRICOS, INGENIEROS ELECTRÓNICOS DE TELECOMUNICACIONES Y AFINES',
 'INGENIEROS INDUSTRIALES Y AFINES',
 'INGENIEROS MEC┴NICOS','AYUDANTE DE TALLER, MECÁNICA, VEHÍCULOS DE MOTOR Y AFINES']

P= 'INGENIEROS'

Q=['COORDINADORES Y SUPERVISORES DE ALMACENAMIENTO, ABASTECIMIENTO Y DISTRIBUCIÓN',
 'COORDINADORES Y SUPERVISORES DE PRODUCCIËN Y OPERACIONES EN CUIDADOS PERSONALES, LIMPIEZA Y SERVICIOS SIMILARES',
 'COORDINADORES Y SUPERVISORES DE PRODUCCIÓN Y OPERACIONES EN CUIDADOS PERSONALES, LIMPIEZA Y SERVICIOS SIMILARES',
 'COORDINADORES Y SUPERVISORES DE VENTAS Y COMERCIALIZACIÓN',
 'COORDINADORES Y SUPERVISORES EN MANDOS MEDIOS DE PRODUCCIÓN Y OPERACIONES EN EMPRESAS PÚBLICAS Y PRIVADAS, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'COORDINADORES Y SUPERVISORES FINANCIEROS Y ADMINISTRATIVOS',
 'OTROS COORDINADORES Y SUPERVISORES EN MANDOS MEDIOS DE EMPRESAS PÚBLICAS Y PRIVADAS, NO CLASIFICADAS BAJO OTROS EPÍGRAFES']

R= 'COORDINADORES'

S= ['CONDUCTORES DE BUSES, MICROBUSES Y COLECTIVOS',
 'CONDUCTORES DE CAMIONES Y VEHÍCULOS PESADOS',
 'CONDUCTORES DE CAMIONETAS Y VEHÍCULOS LIVIANOS',
 'CONDUCTORES DE TAXIS',
 'CONDUCTORES DE VEHÍCULOS ACCIONADOS A PEDAL O A BRAZO',
 'CONDUCTORES DE VEH═CULOS DE TRACCIËN ANIMAL']

T= 'CONDUCTORES'

U= [ 'AUXILIARES ADMINISTRATIVOS Y AFINES',
 'AUXILIARES DE ENFERMERÍA Y ODONTOLOGÍA',
 'AUXILIARES DE LA POLICIA NACIONAL',
'ASISTENTES DE COMERCIO EXTERIOR',
 'ASISTENTES DE ENSEÑANZA EN EDUCACIÓN PREESCOLAR', 'SECRETARIOS (AS)']

V= 'AUXILIARES - ASISTENTES'

W= ['MÉDICOS',
 'MÉDICOS VETERINARIOS Y ZOOTECNISTAS',
 'MÉDICOS, PROFESIONALES EN CIENCIAS DE LA SALUD Y AFINES, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
'ENFERMEROS(AS) PROFESIONALES']

X= 'MÉDICOS'

Y= ['AGRICULTORES DE CULTIVOS PERMANENTES (PLANTACIONES DE ÁRBOLES Y ARBUSTOS)',
 'AGRICULTORES DE CULTIVOS TRANSITORIOS','CRIADORES DE GANADO Y TRABAJADORES DE LA CR═A DE ANIMALES DOM╔STICOS DIVERSOS',
 'TRABAJADORES AGROPECUARIOS',
 'TRABAJADORES DE HUERTAS, INVERNADEROS, VIVEROS Y JARDINES',
 'TRABAJADORES PECUARIOS, GANADEROS Y AFINES, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'TRABAJADORES PECUARIOS, GANADEROS Y AFINES, NO CLASIFICADOS BAJO OTROS EP═GRAFES',]

Z= 'TRABAJOS EN EL CAMPO'

OTRO = ['ABOGADOS',  'ACOMPAÑANTES',
 'ACTIVIDADES NO ESPECIFICADAS',
 'ALBAÑILES, MAMPOSTEROS Y AFINES',
 'ALFAREROS Y AFINES (BARRO, ARCILLA Y ABRASIVOS)',
 'ANALISTAS DE SISTEMAS INFORMÁTICOS',
 'ARQUITECTOS Y URBANISTAS',
 'ARTESANOS DE LA MADERA Y MATERIALES SIMILARES',
 'ASEADORES Y FUMIGADORES DE OFICINAS, HOTELES Y OTROS ESTABLECIMIENTOS',
 'ATLETAS, DEPORTISTAS Y AFINES',
 'BOMBEROS Y RESCATISTAS',
 'BORDADORES Y AFINES',
 'CAJEROS Y EXPENDEDORES DE BILLETES',
 'CARNICEROS, PESCADEROS Y AFINES',
 'CARPINTEROS DE ARMAR Y DE BLANCO',
 'CATADORES Y CLASIFICADORES DE ALIMENTOS Y BEBIDAS',
 'COCINEROS Y AFINES',
 'COMPOSITORES, M┌SICOS Y CANTANTES',
 'COMPRADORES',
 'CONTADORES',
 'DESCANSAR, DORMIR, COMER O PARTICIPAR EN OTRAS ACTIVIDADES VITALES',
 'EBANISTAS Y AFINES',
 'ELECTRICISTAS DE OBRAS Y AFINES',
 'ELECTROTÉCNICOS',
 'ELECTROT╔CNICOS',
 'EMBALADORES MANUALES Y OTROS OBREROS DE LA INDUSTRIA MANUFACTURERA',
 'ENCARGADOS DE SERVICIOS DE TRANSPORTE',
 'ENSAMBLADORES DE MECANISMOS Y ELEMENTOS MECÁNICOS DE MÁQUINAS',
 'ESCRITORES, PERIODISTAS Y AFINES',
 'ESCULTORES, PINTORES Y AFINES',
 'ESPECIALISTAS EN ORGANIZACIÓN, ADMINISTRACIÓN DE EMPRESAS, ANÁLISIS FINANCIERO Y AFINES, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'ESTUDIANTE',
 'FONOAUDÍOLOGOS, FISIOTERAPEUTAS Y AFINES',
 'GUARDIANES DE PRISIÓN',
 'HERREROS Y FORJADORES',
 'HOGAR',
 'INSTALADORES Y REPARADORES DE TELÉGRAFOS, TELÉFONOS Y LÍNEAS ELECTRICAS',
 'JOYEROS, ORFEBRES Y PLATEROS',
 'LIMPIABOTAS Y OTROS TRABAJADORES CALLEJEROS',
 'MARINEROS DE CUBIERTA Y AFINES',
 'MENSAJEROS, PORTEADORES Y REPARTIDORES',
 'MESEROS, TABERNEROS Y AFINES',
 'NIÑERAS Y CUIDADORAS INFANTILES',
 'OCIO',
 'OFICIALES DE LA POLICÍA NACIONAL',
 'OFICIALES DE LAS FMS001',
 'OFICIALES Y OPERARIOS DE LA CONSTRUCCIÓN Y AFINES, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'OTRAS ACTIVIDADES ESPECÍFICAS',
 'OTRO TRABAJO',
 'OTROS ARTESANOS, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'OTROS TRABAJADORES DE SERVICIOS PERSONALES A PARTICULARES, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'OTROS TRABAJADORES DE SERVICIOS PERSONALES A PARTICULARES, NO CLASIFICADOS BAJO OTROS EP═GRAFES',
 'PANADEROS, PASTELEROS Y CONFITEROS',
 'PELUQUEROS, ESPECIALISTAS EN TRATAMIENTOS DE BELLEZA Y AFINES',
 'PENSIONADO',
 'PERSONAL DE LOS SERVICIOS DE PROTECCIËN Y SEGURIDAD, NO CLASIFICADO BAJO OTROS EP═GRAFES',
 'PERSONAL DE LOS SERVICIOS DE PROTECCIÓN Y SEGURIDAD, NO CLASIFICADO BAJO OTROS EPÍGRAFES',
 'PESCADORES',
 'PINTORES DECORADORES DE VIDRIO, CERÁMICA Y OTROS MATERIALES',
 'PINTORES, BARNIZADORES Y ENLACADORES DE ARTÍCULOS METÁLICOS Y AFINES',
 'PINTORES, BARNIZADORES Y ENLACADORES DE ART═CULOS MET┴LICOS Y AFINES',
 'PINTORES, EMPAPELADORES Y AFINES',
 'PROFESIONALES DEL DERECHO, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'PROFESIONALES DEL DERECHO, NO CLASIFICADOS BAJO OTROS EP═GRAFES',
 'PSICÓLOGOS',
 'RECOLECTORES DE MATERIAL RECICLABLE',
 'REPRESENTANTES COMERCIALES Y TÉCNICOS DE VENTAS',
 'REVISORES, GUARDAS Y COBRADORES DE LOS SERVICIOS DE TRANSPORTE',
 'SACERDOTES Y RELIGIOSOS DE DISTINTAS DOCTRINAS',
 'SASTRES, MODISTOS COSTUREROS SOMBREREROS Y AFINES',
 'SOLDADORES Y OXICORTADORES',
 'TRABAJADORES DE LOS CUIDADOS PERSONALES Y AFINES, NO CLASIFICADOS BAJO OTROS EPÍGRAFES',
 'TRABAJADORES DE LOS CUIDADOS PERSONALES Y AFINES, NO CLASIFICADOS BAJO OTROS EP═GRAFES',
 'TRABAJADORES SOCIALES Y AFINES',
 'TRABAJO PARA OBTENER INGRESOS',
 'VIGILANTES Y CELADORES',
 'ZAPATEROS Y AFINES']

OTRAS = 'OTRAS OCUPACIONES ESPECÍFICAS'

#Ahora generamos las nuevas categorías
BDD['OCUPACION'] = BDD['OCUPACION'].replace(A,B)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(C,D)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(E,F)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(G,H)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(I,J)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(K,L)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(M,N)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(O,P)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(Q,R)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(S,T)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(U,V)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(W,X)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(Y,Z)
BDD['OCUPACION'] = BDD['OCUPACION'].replace(OTRO,OTRAS)
BDD['OCUPACION'] = BDD['OCUPACION'].replace({'SIN INFORMACION':'SIN INFORMACIÓN'})

#Verificamos ajustes
sorted(BDD['OCUPACION'].unique())

"""##Continuación de verificación y limpieza de variables"""

#Agregamos la información a los datos vacíos de la columna "Cultura, pueblo, rasgos físicos" que tiene 22 datos nulos, con la anotación "SIN INFORMACIÓN"
BDD['Cultura, pueblo, rasgos físicos'].fillna('SIN INFORMACIÓN', inplace=True)

#Verificamos categorías de la columna
sorted(BDD['Cultura, pueblo, rasgos físicos'].unique())

#Vamos a unificar algunas categorías
BDD['Cultura, pueblo, rasgos físicos'] = BDD['Cultura, pueblo, rasgos físicos'].replace({'NEGRO(A), MULATO(A), AFRO COLOMBIANO(A) O AFRO DESCENDIENTE': 'NEGRO(A), MULATO(A), AFROCOLOMBIANO(A) O AFRODESCENDIENTE',
                                                   'NINGUNA DE LAS ANTERIORES':'NINGUNA',
                                                   'NINGUNO DE LOS ANTERIORES':'NINGUNA'})

#Verificamos ajustes
sorted(BDD['Cultura, pueblo, rasgos físicos'].unique())

#Como los datos son aplicados a Colombia y las variables "País de residencia", "Departamento de residencia" y "Municipio de residencia" tienen 2 datos nulos cada una, se procede a eliminar estas dos filas para cada una.
BDD = BDD.dropna(subset=['País de residencia', 'Departamento de residencia', 'Municipio de residencia'])

#Verificamos datos nulos
BDD.isnull().sum()

#Verificamos categorías de la columna DE MUNICIPIO RESIDENCIA
sorted(BDD['Municipio de residencia'].unique())

#Vamos a unificar algunas categorías
BDD['Municipio de residencia'] = BDD['Municipio de residencia'].replace({'BOLÍVAR':'BOLIVAR',
                                                                              'BUGA':'GUADALAJARA DE BUGA',
                                                                              'ALTOS DE GUADALAJARA':'ALTOS DEL GUADALAJARA',
                                                                              'CALI (SANTIAGO DE CALI)':'CALI',
                                                                              'CALIMA (DARIEN)':'CALIMA',
                                                                              'GUACARÍ':'GUACARI',
                                                                              'JAMUNDÍ':'JAMUNDI',
                                                                              'LA UNIÓN':'LA UNION',
                                                                              'RIOFRÍO':'RIOFRIO',
                                                                              'TULUA':'TULUÁ'})
#Verificamos ajustes
sorted(BDD['Municipio de residencia'].unique())

#Verificamos categorías de la columna DE DEPARTAMENTO RESIDENCIA
sorted(BDD['Departamento de residencia'].unique())

#Vamos a unificar algunas categorías
BDD['Departamento de residencia'] = BDD['Departamento de residencia'].replace({'QUINDIO':'QUINDÍO'
})
#Verificamos ajustes
sorted(BDD['Departamento de residencia'].unique())

#Agregamos la información a los datos vacíos de la columna "Area de residencia" que tiene 4 datos nulos, con la anotación "SIN INFORMACIÓN"
BDD['Area de residencia'].fillna('SIN INFORMACIÓN', inplace=True)

#Verificamos categorías de la columna
sorted(BDD['Area de residencia'].unique())

#Vamos a unificar algunas categorías
BDD['Area de residencia'] = BDD['Area de residencia'].replace({'"CENTRO POBLADO (Inspección,\r\ncorregimiento o caserío)"':'CENTRO POBLADO (INSPECCIÓN, CORREGIMIENTO O CASERÍO)'})

#Verificamos ajustes
sorted(BDD['Area de residencia'].unique())

#Agregamos la información a los datos vacíos de la columna "Barrio de fallecido" que tiene 210 datos nulos, con la anotación "SIN INFORMACIÓN"
BDD['Barrio de fallecido'].fillna('SIN INFORMACIÓN', inplace=True)

#Verificamos categorías de la columna
sorted(BDD['Barrio de fallecido'].unique())

#Vamos a unificar algunas categorías
BDD['Barrio de fallecido'] = BDD['Barrio de fallecido'].replace({'ACUARELA':'ACUARELAS',
                                                                 'ALTO BONITO':'ALTOBONITO',
                                                                 'ALTOS DE GUADALAJARA':'ALTOS DEL GUADALAJARA',
                                                                 'BALCONESL DEL NORTE':'BALCONES DEL NORTE',
                                                                 'BELLAVISTA':'BELLA VISTA',
                                                                 'DIVINNO NIÑO':'DIVINO NIÑO',
                                                                 'DIVINO NINO':'DIVINO NIÑO',
                                                                 'EL DIVINO NIÑO':'DIVINO NIÑO',
                                                                 'ENTRE VALLES':'ENTREVALLES',
                                                                 'FUENMAYOR': 'FUEN MAYOR',
                                                                 'GONZALO ECHEVERRI':'GONZALO ECHEVERRY',
                                                                 'JORGE ELIZER GAITAN':'JORGE ELIECER GAITAN',
                                                                 'LA CONFORNIA':'LA CONCORDIA',
                                                                 'LA CORCORDIA':'LA CONCORDIA',
                                                                 'LA MARCED':'LA MERCED',
                                                                 'LAS MERCED': 'LA MERCED',
                                                                 'LA REVOLUCION':'LA REVOLUCIÓN',
                                                                 'LA REVOLUSION':'LA REVOLUCIÓN',
                                                                 'REVOLUCION':'LA REVOLUCIÓN',
                                                                 'MARA LUISA DE LA ESPADA': 'MARIA LUISA  LA ESPADA',
                                                                 'MARIA LUISA': 'MARIA LUISA DE LA ESPADA',
                                                                 'MARIA LUISA  LA ESPADA':'MARIA LUISA DE LA ESPADA',
                                                                 'MARIA LUISA LA EPSADA':'MARIA LUISA DE LA ESPADA',
                                                                 'MARIA LUISA LA ESPADA':'MARIA LUISA DE LA ESPADA',
                                                                 'MARIA LUISA  LA ESPADA':'MARIA LUISA DE LA ESPADA',
                                                                 'NO RECUERDA':'NO IDENTIFICADO',
                                                                 'NO SABE':'NO IDENTIFICADO',
                                                                 'OTROS':'OTRO',
                                                                 'OTROS BARRIOS':'OTRO',
                                                                 'SD':'OTRO',
                                                                 'RICAUTE':'RICAURTE',
                                                                 'SAN JOSE DE LAS PALMAS':'SAN JOSÉ DE LAS PALMAS',
                                                                 'SIN DATOS': 'SIN INFORMACIÓN',
                                                                 'SIN INFORMACION':'SIN INFORMACIÓN',
                                                                 'UNI NORTE': 'UNINORTE'})

#Verificamos ajustes
sorted(BDD['Barrio de fallecido'].unique())

#Verificamos categorías de la columna "Vereda del fallecido" que no tiene datos nulos
sorted(BDD['Vereda del fallecido'].unique())

#Vamos a unificar algunas categorías
BDD['Vereda del fallecido'] = BDD['Vereda del fallecido'].replace({'CHAMBINBAL':'CHAMBIMBAL',
                                                                    'CHAMBINBAL SAN ANTONIO':'CHAMBIMBAL SAN ANTONIO',
                                                                    'MEDIACANOA':'MEDIA CANOA',
                                                                    'MIRAVLLE':'MIRAVALLE',
                                                                    'NO DIENTIFICADA':'NO IDENTIFICADA',
                                                                    'SIN INFORMACION':'SIN INFORMACIÓN',
                                                                    'chambimbal la campiña':'CHAMBIMBAL LA CAMPIÑA',
                                                                   'el manantial':'EL MANANTIAL',
                                                                   'el vinculo':'EL VINCULO'})
#Verificamos ajustes
sorted(BDD['Vereda del fallecido'].unique())

#Agregamos la información a los datos vacíos de la columna "Seguridad social" que tiene 27 datos nulos, con la anotación "SIN INFORMACIÓN"
BDD['Seguridad social'].fillna('SIN INFORMACIÓN', inplace=True)

#Verificamos categorías de la columna
sorted(BDD['Seguridad social'].unique())

#Verificamos categorías de la columna "Entidad administradora de salud" que no tiene datos nulos
sorted(BDD['Entidad administradora de salud'].unique())

#Vamos a unificar algunas categorías
BDD['Entidad administradora de salud'] = BDD['Entidad administradora de salud'].replace({'ASMET SALUD EPS SAS -CM':'ASMET SALUD EPS SAS',
                                                                                         'ASMET SALUD ESS - ASOCIACION MUTUAL LA ESPERANZA':'ASMET SALUD EPS SAS',
                                                                                         'ASOCIACIÓN MUTUAL LA ESPERANZA - ASMET SALUD':'ASMET SALUD EPS SAS',
                                                                                         'ASOCIACIÓN MUTUAL LA ESPERANZA - ASMET SALUD -CM':'ASMET SALUD EPS SAS',
                                                                                         'ASOCIACIÓN MUTUAL LA ESPERANZA ASMET  SALUD-CM':'ASMET SALUD EPS SAS',
                                                                                         'ASOCIACIÓN MUTUAL EMPRESA SOLIDARIA DE SALUD DE NARIÑO - EMSSANAR E.S.S. -CM':'EMSSANAR S.A.S.',
                                                                                         'ASOCIACIÓN MUTUAL EMPRESA SOLIDARIA DE SALUD DE NARIÑO E.S.S. EMSSANAR E.S.S.-CM':'EMSSANAR S.A.S.',
                                                                                         'CAFESALUD E.P.S.':'CAFESALUD E.P.S.  S.A.',
                                                                                         'COMFENALCO VALLE':'COMFENALCO  VALLE  E.P.S.',
                                                                                         'COMPENSAR   E.P.S.-CM':'COMPENSAR   E.P.S.',
                                                                                         'COMPENSAR E.P.S.':'COMPENSAR   E.P.S.',
                                                                                         'COMPENSAR E.P.S. -CM':'COMPENSAR   E.P.S.',
                                                                                         'COOMEVA   E.P.S.  S.A.':'COOMEVA E.P.S. S.A.',
                                                                                         'COOMEVA E.P.S. S.A. -CM':'COOMEVA E.P.S. S.A.',
                                                                                         'COOMEVA MEDICINA PREPAGADA S.A.':'COOMEVA E.P.S. S.A.',
                                                                                         'COOPERATIVA DE SALUD Y DESARROLLO INTEGRAL ZONA SUR ORIENTAL DE CARTAGENA LTDA. - COOSALUD E.S.S. -CM':'COOPERATIVA DE SALUD Y DESARROLLO INTEGRAL ZONA SUR ORIENTAL DE CARTAGENA LTDA. - COOSALUD E.S.S.',
                                                                                         'E.P.S. SANITAS':'E.P.S.  SANITAS  S.A.',
                                                                                         'EPS SANITAS - CM':'E.P.S.  SANITAS  S.A.',
                                                                                         'EMSSANAR ESS - ASOCIACION MUTUAL EMPRESA SOLIDARIA DE SALUD':'EMSSANAR S.A.S.',
                                                                                         'ENTIDAD COOPERATIVA SOLIDARIA DE SALUD DEL NORTE DE SOACHA - ECOOPSOS -CM':'ENTIDAD COOPERATIVA SOLIDARIA DE SALUD DEL NORTE DE SOACHA - ECOOPSOS',
                                                                                         'EPS S.O.S. S.A. - EPS SERVICIO OCCIDENTAL DE SALUD  S.A.': 'EPS SERVICIO OCCIDENTAL DE SALUD  S.A. - EPS S.O.S. S.A.-CM',
                                                                                         'EPS Y MEDICINA PREPAGADA SURAMERICANA S.A': 'EPS SURA',
                                                                                         'EPS Y MEDICINA PREPAGADA SURAMERICANA S.A -CM': 'EPS SURA',
                                                                                         'EPS Y MEDICINA PREPAGADA SURAMERICANA S.A-CM': 'EPS SURA',
                                                                                         'EPS Y MEDICINA PREPAGADA SURAMERICANA S.A.': 'EPS SURA',
                                                                                         'FAMISANAR E.P.S. LTDA - CAFAM - COLSUBSIDIO -CM':'FAMISANAR E.P.S. LTDA - CAFAM - COLSUBSIDIO',
                                                                                         'MEDIMÁS EPS S.A.S. CONTRIBUTIVO':'MEDIMÁS EPS S.A.S.',
                                                                                         'MEDIMÁS EPS S.A.S. SUBSIDIADO':'MEDIMÁS EPS S.A.S.',
                                                                                         'MEDIMÁS EPS S.A.S. -CM':'MEDIMÁS EPS S.A.S.',
                                                                                         'LA NUEVA EPS S.A.-CM':'NUEVA EPS S.A',
                                                                                         'NUEVA EPS S.A.':'NUEVA EPS S.A',
                                                                                         'NUEVA EPS S.A. -CM':'NUEVA EPS S.A',
                                                                                         'NUEVA EPS SA':'NUEVA EPS S.A',
                                                                                         'SALUD TOTAL E.P.S. -CM':'SALUD TOTAL S.A.',
                                                                                         'SALUD TOTAL S.A. ENTIDAD PROMOTORA DE SALUD':'SALUD TOTAL S.A.',
                                                                                         'SAVIA SALUD E.P.S. -CM': 'SAVIA SALUD E.P.S.',
                                                                                         'SERVICIO OCCIDENTAL DE SALUD - S.O.S. S.A. -CM': 'SERVICIO OCCIDENTAL DE SALUD - S.O.S. S.A.'})
#Verificamos ajustes
sorted(BDD['Entidad administradora de salud'].unique())

#Agregamos la información a los datos vacíos de la columna "Probable manera de muerte" que tiene 640 datos nulos, con la anotación "SIN INFORMACIÓN"
BDD['Probable manera de muerte'].fillna('SIN INFORMACIÓN', inplace=True)

#Verificamos categorías de la columna
sorted(BDD['Probable manera de muerte'].unique())
#No hay más ajustes para esta columna

#Se observa la categoría "NO NATURAL" y esta podría ser más bien un agrupador de las categorías 'ACCIDENTE',  'HOMICIDIO',  'SUICIDIO (AUTOLESIÓN)',
#Por lo tanto verificamos si contiene muchas características, de lo contrario procedería a eliminarse de la base.
BDD['Probable manera de muerte'].value_counts()

#Como no tiene demasiadas características y no puede asociarse a otra categoría, se elimina de la base
BDD = BDD[BDD['Probable manera de muerte'] != 'NO NATURAL']

#Además, agrupamos las categorías: PENDIENTE DE INVESTIGACIÓN Y "SIN INFORMACIÓN"
BDD['Probable manera de muerte'] = BDD['Probable manera de muerte'].replace({'PENDIENTE DE INVESTIGACIÓN':'SIN INFORMACIÓN'})

#Verificamos
BDD['Probable manera de muerte'].value_counts()

#Como las variables "longitud", "latitud" y localización pueden aportarnos información espacial visual de los datos, se conservan y se reemplazan los datos nulos con cero (894 datos nulos cada uno)
BDD['longitud'].fillna(0, inplace=True)
BDD['latitud'].fillna(0, inplace=True)
BDD['Geolocalización'].fillna('NA', inplace=True)

#Confirmamos el tipo object para la variable de geolocalización
BDD['Geolocalización'] = BDD['Geolocalización'].astype(object)

#Verificamos los datos nulos
BDD.isnull().sum()

#No quedan datos nulos en las variables, y para garantizar que la información de las variables "año" y "mes" sean correctas, se reemplaza la información de estas columnas a partir de la variable "Fecha de Defunción"
# Extraer el nombre del mes y agregarlo en la columna 'mes'
BDD['mes'] = BDD['Fecha de Defunción'].dt.strftime('%B')
# Extraer el año y agregarlo en columna 'año'
BDD['año'] = BDD['Fecha de Defunción'].dt.year

#Verificamos el contenido de las variables
BDD[['año', 'mes']]

#Hemos finalizado la limpieza y preprocesamiento de todas las variables en la base de datos.

#Verificamos las dimensiones finales del conjunto de datos
BDD.shape

"""#Exploración y análisis de datos"""

#ahora haremos una exploración rápida de las variables
EXP= sv.analyze(BDD)
EXP.show_notebook()

"""Teniendo en cuenta la exploración rápida de los datos para cada variable, tenemos lo siguiente: Todos los datos de defunciones de la base, son de personas que vivían y murieron en Colombia, del departamento del Valle del Cauca, principalmente en el municipio de Guadalajara de Buga con el 99%, sin embargo sus municipios de residencia variaban entre Buga con el 73% y Guacari con un 7%. La principal área de defunción ha sido la cabecera municipal con el 96% de los casos en diferentes barrios, 2% en área rural dispersa y 1% en un centro poblado.
En cuanto al sitio específico de defunción, el 64% ha fallecido en el hospital, el 30% en su casa, 5% en vías públicas y el otro 1% en otros lugares.
Las instituciones de salud en las que se han presentado mayor cantidad de fallecidos son la Fundación Hospital San José con el 74% de los casos, 6% en la E.S.E. Hospital Divino Niño y hay un 15% sin información.
Para el tipo de defunción, se ha tenido un 100% de defunciones de tipo No fetal, la mayoría de fallecidos han sido del sexo Masculino con el 56%, el 25% de fallecidos estaban viudos, el 23% estaban casados, el 22% solteros. La mayoría de fallecidos eran mayores de 50 años de edad, su nivel educativo era principalmente básica primaria (55%) y en segundo lugar básica secundaria con 15%. El 30% se ocupaba de su hogar y se tiene un 42% sin esta información de ocupación. En cuanto a rasgos físicos y cultura, la mayoría no tenía alguna clasificación (98%), solo un 2% era afrocolombiano y menos del 1% indígena.

La mayoría de fallecidos contaban con seguridad social: 48% Contributivo y 48% subsidiado, la EPS era principalmente Nueva EPS con el 31% y luego EMSSANAR E.S.S. con el 17%.

La muerte natural por enfermedad con un 65% es la que más se presentó. Se tiene un 25% de los casos sin información.

Los datos son principalmente de los años 2021 y 2022 y los meses con mayor cantidad d efallecidos fueron los meses de Julio y enero con 11% cada uno y luego Junio con el 10% de los casos.
"""

# crear dataset para Cantidad de defunciones por Sexo y por año

BASE_A = BDD.groupby(['Sexo','año'])[['Fecha de Defunción']].count().reset_index()

# crear gráfica
fig_A = px.bar(BASE_A, x = 'año', y ='Fecha de Defunción', color ='Sexo', barmode = 'group')

# agregar detalles a la gráfica
fig_A.update_layout(
    title = '<b>Cantidad de defunciones por Sexo y por año<b>',
    xaxis_title = 'Año de defunción',
    yaxis_title = 'Número de defunciones',
    template = 'simple_white',
    title_x = 0.5)

fig_A.show()

"""Tanto en 2021 como en 2022, la mayoría de fallecidos fueron del sexo masculino."""

# Crear dataset para Cantidad de defunciones por la Probable manera de muerte y por año
BASE_C = BDD.groupby(['Probable manera de muerte', 'año'])[['Fecha de Defunción']].count().reset_index()

# Crear gráfica
fig_C = px.bar(BASE_C, x='año', y='Fecha de Defunción', color='Probable manera de muerte', barmode='relative')

# Agregar detalles a la gráfica
fig_C.update_layout(
    title='<b>Cantidad de defunciones por la Probable manera de muerte y por año<b>',
    xaxis_title='Año de defunción',
    yaxis_title='Número de defunciones',
    template='simple_white',
    title_x=0.5
)

fig_C.show()

"""Tanto en 2021 como en 2022, la mayoría de muertes fueron de manera natural por enfermedad, sin embargo hay una buena proporción sin información en el 2022. En 2021 se dió una porción de homicidios y en 2022 se cataloga como muerte no natural."""

import plotly.express as px

# Crear dataset para Cantidad de defunciones por Edad y por año
BASE_E = BDD.groupby(['Nivel educativo', 'Estado civil'])[['Fecha de Defunción']].count().reset_index()

# Crear gráfica
fig_E = px.bar(BASE_E, x='Estado civil', y='Fecha de Defunción', color='Nivel educativo', barmode='stack')

# Agregar detalles a la gráfica
fig_E.update_layout(
    title='<b>Cantidad de defunciones por Estado civil y por Nivel educativo<b>',
    xaxis_title='Estado civil',
    yaxis_title='Número de defunciones',
    template='simple_white',
    title_x=0.5
)

fig_E.show()

# crear dataset para Cantidad de defunciones por mes y por Probable manera de muerte

BASE_F = BDD.groupby(['mes','Probable manera de muerte'])[['Fecha de Defunción']].count().reset_index()

# crear gráfica
fig_F = px.bar(BASE_F, x = 'mes', y ='Fecha de Defunción', color ='Probable manera de muerte', barmode = 'stack')

# agregar detalles a la gráfica
fig_F.update_layout(
    title = '<b>Cantidad de defunciones por mes y por Probable manera de muerte<b>',
    xaxis_title = 'Probable manera de muerte',
    yaxis_title = 'Número de defunciones',
    template = 'simple_white',
    title_x = 0.5)

"""la mayoría de meses domina la muerte por enfermedad, en todos está presente  la muerte por homicidio y una baja proporción por accidentes.

> Bloque con sangría


"""

# crear dataset para Cantidad de defunciones porEdad y por Probable manera de muerte

BASE_G = BDD.groupby(['Edad','Probable manera de muerte'])[['Fecha de Defunción']].count().reset_index()

# crear gráfica
fig_G = px.bar(BASE_G, x = 'Probable manera de muerte', y ='Fecha de Defunción', color ='Edad', barmode = 'group')

# agregar detalles a la gráfica
fig_G.update_layout(
    title = '<b>Cantidad de defunciones por Edad y por Probable manera de muerte<b>',
    xaxis_title = 'Probable manera de muerte',
    yaxis_title = 'Número de defunciones',
    template = 'simple_white',
    title_x = 0.5)

fig_G.show()

"""Cuando se observa la información pór manera de morir y su distribución de edades, se puede decir que la muerte por enfermedad predomina en personas mayores de 50 años, mientras que para el resto de maneras predominan edades inferiores a 50 años."""

#Veamos la Entidad administradora de salud para las defunciones cuya "probable manera de muerte" fue la "NATURAL (ENFERMEDAD)"
# crear dataset para MUERTE NATURAL

base_H = BDD[BDD['Probable manera de muerte'].isin(['NATURAL (ENFERMEDAD)'])]
base_H = base_H.groupby(['Probable manera de muerte','Entidad administradora de salud'])[['Fecha de Defunción']].count().reset_index()

# crear gráfica
fig_H = px.bar(base_H, x='Entidad administradora de salud', y='Fecha de Defunción', color ='Probable manera de muerte', barmode ='group', title ='<b>Entidad administradora de salud para las defunciones cuya "probable manera de muerte" fue la "NATURAL (ENFERMEDAD)"<b>')

# agregar detalles a la gráfica
fig_H.update_layout(
    xaxis_title = 'Entidad administradora de salud',
    yaxis_title = 'N° de defunciones',
    template = 'simple_white',
    title_x = 0.5)

fig_H.show()

"""Cuando se quiere ver la información de fallecidos por enfermedad y su EPS, se tiene que la mayoría pertenecían a La Nueva EPS, y EMSSANAR"""

# Matriz de correlación
plt.figure(figsize=(20,10))
c= BDD.corr()
sns.heatmap(c, annot=True)
#Solo se observa una correlación baja entre la edad y el nivel de compromiso.

"""#Preparación de los datos para aplicar algoritmos

##Creación de dataset
"""

## Vamos a trabajar con las siguientes variables de la base de datos
## La primera línea es de variables categóricas y la segunda línea de las variables numéricas

VarNuevas=['mes','Área defunción','Sexo', 'Probable manera de muerte', 'Seguridad social', 'Estado civil', 'Nivel educativo',
           'Edad', 'año']

#Generamos entonces una copia de estas variables en una nueva base de datos:
BDD1 = BDD[VarNuevas].copy()

#Visualizamos la nueva base
BDD1.head()

"""#Preprocesamiento de los datos

##Metodología 1: Codificación de las categorías de las variables
"""

# Metodología 1 - Codificación de las categorías de las variables
#Hacemos una copia de la base de datos
BDD2 = BDD1.copy()

#Variable Mes: puede convertirse en una variable de tipo numérica de acuerdo con su naturaleza, ya que se puede ordenarse por temporalidad
#Para lo cual convertiremos en la columna "MesN"

BDD2['mes'] = BDD1['mes'].map({'January': 1, 'February': 2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7, 'August':8, 'September': 9, 'October':10, 'November':11, 'December':12})

#Variable Área de defunción: La codificaremos de mayor a menor cantidad de población.
BDD2['Área defunción'] = BDD1['Área defunción'].map({'SIN INFORMACIÓN':0,'CABECERA MUNICIPAL':1, 'CENTRO POBLADO':2, 'RURAL DISPERSO':3})

#Variable sexo: se codifica similar a si fuera dummies
BDD2['Sexo'] = BDD1['Sexo'].map({'FEMENINO':0, 'MASCULINO':1})

#Variable Probable manera de muerte: Se codificará con asignación personalizada de lo que podría considerarse menos grave hacia lo más grave de acuerdo con una consulta sobre criminalística en Colombia.

BDD2['Probable manera de muerte'] = BDD1['Probable manera de muerte'].map({'SIN INFORMACIÓN': 0,
                                                   'NATURAL (ENFERMEDAD)':1,
                                                   'ACCIDENTE':2,
                                                   'SUICIDIO (AUTOLESIÓN)':3,
                                                   'HOMICIDIO':4})

#Variable Seguridad social: puede convertirse en una variable de tipo numérica de acuerdo con su naturaleza, ya que se puede ordenar menor a mayor nivel de seguridad social utilizando categorías numéricas
#Para lo cual la convertiremos en la columna "Nivel SS"

BDD2['Seguridad social'] = BDD1['Seguridad social'].map({'SIN INFORMACIÓN': 0,
                                                   'NO ASEGURADO':1,
                                                   'EXCEPCIÓN':2,
                                                   'SUBSIDIADO':3,
                                                   'CONTRIBUTIVO':4,
                                                   'ESPECIAL':5})

#Variable Estado civil: puede convertirse en una variable de tipo numérica de acuerdo con su naturaleza, ya que se puede ordenar menor a mayor nivel de compromiso que representa el estado civil, utilizando categorías numéricas
#Para lo cual convertimos en la columna "Nivel compromiso civil"

BDD2['Estado civil'] = BDD1['Estado civil'].map({'SIN INFORMACIÓN': 0,
                                                           'SOLTERO(A)':1,
                                                           'UNIÓN LIBRE':2,
                                                           'CASADO(A)':3,
                                                           'SEPARADO (A)':4,
                                                           'VIUDO(A)':5})

#Variable Nivel educativo: puede convertirse en una variable de tipo numérica de acuerdo con su naturaleza, ya que se puede ordenar menor a mayor nivel de estudios utilizando categorías numéricas
#Para lo cual convertiremos en la columna "Nivel educación"

BDD2['Nivel educativo'] = BDD1['Nivel educativo'].map({'SIN INFORMACIÓN': 0,
                                                       'NINGUNO':1,
                                                       'PREESCOLAR':2,
                                                       'BÁSICA PRIMARIA':3,
                                                       'BÁSICA SECUNDARIA':4,
                                                       'MEDIA ACADÉMICA':5,
                                                       'MEDIA TÉCNICA':6,
                                                       'TÉCNICA PROFESIONAL':7,
                                                       'TECNOLÓGICA':8,
                                                       'PROFESIONAL':9,
                                                       'ESPECIALIZACION':10,
                                                       'MAESTRÍA':11})

BDD2.head()

"""##Metodología 2. POR FRECUENCIAS"""

#Metodología 2. POR FRECUENCIAS
#Hacemos una copia de la base de datos
BDD3 = BDD1.copy()

#UTILIZAMOS LA MISMA CODIFICACIÓN QUE EN LA METODOLOGÍA ANTERIOR, EXCEPTO PARA LA VARIABLE "Probable manera de muerte", a la cual le calcularemos frecuencia

#Variable Mes: puede convertirse en una variable de tipo numérica de acuerdo con su naturaleza, ya que se puede ordenarse por temporalidad
#Para lo cual convertiremos en la columna "MesN"

BDD3['mes'] = BDD1['mes'].map({'January': 1, 'February': 2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7, 'August':8, 'September': 9, 'October':10, 'November':11, 'December':12})

#Variable Área de defunción: La codificaremos de mayor a menor cantidad de población.
BDD3['Área defunción'] = BDD1['Área defunción'].map({'SIN INFORMACIÓN':0,'CABECERA MUNICIPAL':1, 'CENTRO POBLADO':2, 'RURAL DISPERSO':3})

#Variable sexo: se codifica similar a si fuera dummies
BDD3['Sexo'] = BDD1['Sexo'].map({'FEMENINO':0, 'MASCULINO':1})

#Variable Seguridad social: puede convertirse en una variable de tipo numérica de acuerdo con su naturaleza, ya que se puede ordenar menor a mayor nivel de seguridad social utilizando categorías numéricas
#Para lo cual la convertiremos en la columna "Nivel SS"

BDD3['Seguridad social'] = BDD1['Seguridad social'].map({'SIN INFORMACIÓN': 0,
                                                   'NO ASEGURADO':1,
                                                   'EXCEPCIÓN':2,
                                                   'SUBSIDIADO':3,
                                                   'CONTRIBUTIVO':4,
                                                   'ESPECIAL':5})

#Variable Estado civil: puede convertirse en una variable de tipo numérica de acuerdo con su naturaleza, ya que se puede ordenar menor a mayor nivel de compromiso que representa el estado civil, utilizando categorías numéricas
#Para lo cual convertimos en la columna "Nivel compromiso civil"

BDD3['Estado civil'] = BDD1['Estado civil'].map({'SIN INFORMACIÓN': 0,
                                                           'SOLTERO(A)':1,
                                                           'UNIÓN LIBRE':2,
                                                           'CASADO(A)':3,
                                                           'SEPARADO (A)':4,
                                                           'VIUDO(A)':5})

#Variable Nivel educativo: puede convertirse en una variable de tipo numérica de acuerdo con su naturaleza, ya que se puede ordenar menor a mayor nivel de estudios utilizando categorías numéricas
#Para lo cual convertiremos en la columna "Nivel educación"

BDD3['Nivel educativo'] = BDD1['Nivel educativo'].map({'SIN INFORMACIÓN': 0,
                                                       'NINGUNO':1,
                                                       'PREESCOLAR':2,
                                                       'BÁSICA PRIMARIA':3,
                                                       'BÁSICA SECUNDARIA':4,
                                                       'MEDIA ACADÉMICA':5,
                                                       'MEDIA TÉCNICA':6,
                                                       'TÉCNICA PROFESIONAL':7,
                                                       'TECNOLÓGICA':8,
                                                       'PROFESIONAL':9,
                                                       'ESPECIALIZACION':10,
                                                       'MAESTRÍA':11})

# Calcular la frecuencia de cada categoría Probable manera de muerte
frecuencia = BDD3['Probable manera de muerte'].value_counts(normalize=True)

# Aplicar la codificación de frecuencia
BDD3['Probable manera de muerte'] = BDD3['Probable manera de muerte'].map(frecuencia)

BDD3.head()

"""##Verificar las nuevas variables y ajustarlas"""

#Verificamos la dimensionalidad ede la base con la metodología 1, con la cual seguiremos trabajando
BDD2.shape

#Verificamos graficamente para analizar

BDD2.hist(figsize=(14,10), bins=15)
plt.tight_layout()
plt.show()

# Escalamos las variables numéricas
train_cols = BDD2[['Edad', 'año']]

scaler = StandardScaler()
scaled_BDD2 = pd.DataFrame(scaler.fit_transform(train_cols), columns = train_cols.columns)
scaled_BDD2.head(3)

"""### Tratamiento de valores atípicos"""

# Tratamiento de valores atípicos
BDD2_out= BDD2.copy()

# Sentar las bases de un "mosaico boxplot de 3x3 (para cubrir 9 variables)":
box_locs=pd.DataFrame(
{"vert":sorted(list(range(0,3))*3),
 "horiz":list(range(0,3))*3}
)

fig,axs=plt.subplots(ncols=3,nrows=3,figsize=(20,20))

for idx,i in enumerate(BDD2_out.columns):
    sns.boxplot(y=BDD2_out[i],ax=axs[box_locs.iloc[idx,0]][box_locs.iloc[idx,1]])

# Tratamos las variables que muestran valores atípicos de acuerdo con los gráficos y cuyos valores no afectarían directamente las categorías.
outs_idx=[BDD2_out[BDD2_out['Nivel educativo'] < 1]['Nivel educativo'].index.astype('int'),
          BDD2_out[BDD2_out['año'] < 2021]['año'].index.astype('int'),
          BDD2_out[BDD2_out['Edad'] < 15]['Edad'].index.astype('int')]

# Seleccionar los únicos y ordenarlos:
outs_idx=pd.Series(sorted(np.concatenate(outs_idx))).unique()

# Eliminar estas filas de 'BDD2_out' y 'BDD2, y RESTABLECER LOS ÍNDICES después:
BDD2_out.drop(index=outs_idx,inplace=True)
BDD2_out.reset_index(drop=True,inplace=True) # 101174 rows left

#Verificamos las dimensiones de la base antes de tratar los valores atípicos
BDD2.shape

#Verificamos las dimensiones después de tratar los valores atípicos
BDD2_out.shape

"""##Reducción de la dimensionalidad"""

#Cree un dataset reducido que proporcione el 100% de la varianza de entrada que debe explicarse
pca = PCA(n_components=1).fit(BDD2_out)

#Veamos cuántas componentes ha conservado PCA:
pca.n_components_

# --- Varianza explicada ---
PCA_variance = pd.DataFrame({'Varianza explicada (%)':
                             pca.explained_variance_ratio_*100})

fig, ax = plt.subplots(1, 1, figsize = (7, 5))

bar = sns.barplot(x = ['PC ' + str(i) for i in range(1)],
                  y = PCA_variance['Varianza explicada (%)'],
                  linewidth = 1.5, edgecolor = 'k', color = '#4bafb8',
                  alpha = 0.8)

plt.show()

PCA_variance
##Este componente explica el 94,3% de la varianza

#Por último, transforme "BDD2" de 9 columnas X_pca
X_pca = pca.fit_transform(BDD2_out)
X_pca

"""#Clustering Algorithm: K-Means

##Dataset Escalado
"""

# Número optimo de K
inertia_list = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=3)
    kmeans.fit(BDD2_out)
    inertia_list.append(kmeans.inertia_)

# plot the inertia curve
plt.plot(range(1,11),inertia_list)
plt.scatter(range(1,11),inertia_list)
plt.xlabel("Number of Clusters", size=10)
plt.ylabel("Inertia Value", size=10)
plt.title("Different Inertia Values for Different Number of Clusters", size=12)
plt.show()

# Selección automatica del numero k
kl = KneeLocator(range(1, 11),
                 inertia_list, curve="convex",
                 direction="decreasing")
kl.elbow

#El número de clusters seleccionados es:

kmeans_constants = {"init": "random", "n_init": 100, "max_iter": 500, "random_state": 42}

# --- Modelo K-means --- para 3 clusters seleccionados anteriormente
model_kmeans = KMeans(n_clusters = 3, **kmeans_constants)
model_kmeans.fit(BDD2_out)

# --- Evaluación del modelo kmeans ---
print("K-MEANS")
print('Inertia: ', model_kmeans.inertia_)
print('Silhouette Score: ', silhouette_score(BDD2_out, model_kmeans.labels_))
print('Calinski harabasz score: ', calinski_harabasz_score(BDD2_out, model_kmeans.labels_))

"""**-La baja inercia indica clusters compactos.**

**-La alta puntuación de racimos de silueta indica que están bien definidas.**

**-El alto puntaje Calinski-Harabasz sugiere clusters bien separados.**
"""

# Predicción de los clusters
Cluster_1 = model_kmeans.fit_predict(BDD2_out)
Cluster_1

# Vamos a adjuntar 'Cluster_1' como una nueva variable a 'BDD2_out':
BDD_final = BDD2_out.copy()
BDD_final['Cluster_1KM_DE']= Cluster_1

# A partir de ahora, sólo trabajaremos con "BDD_final"
BDD_final.head()

BDD_final.Cluster_1KM_DE.unique()   ##Agregar etiquetas a los clusters

"""##Dataset Reducido"""

# Número optimo de K
inertia_list = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=3)
    kmeans.fit(X_pca)
    inertia_list.append(kmeans.inertia_)

# plot the inertia curve
plt.plot(range(1,11),inertia_list)
plt.scatter(range(1,11),inertia_list)
plt.xlabel("Number of Clusters", size=10)
plt.ylabel("Inertia Value", size=10)
plt.title("Different Inertia Values for Different Number of Clusters", size=12)
plt.show()

# Seleccion automatica del numero k
kl = KneeLocator(range(1, 11),
                 inertia_list, curve="convex",
                 direction="decreasing")
kl.elbow
##Igualmente el número de cluster es 3

kmeans_constants = {"init": "random", "n_init": 100, "max_iter": 500, "random_state": 42}

# --- Modelo K-means ---
model_kmeans_pca = KMeans(n_clusters = 3, **kmeans_constants)
model_kmeans_pca.fit(X_pca)

# --- Evaluación del modelo kmeans ---
print("K-MEANS")
print('Inertia: ', model_kmeans_pca.inertia_)
print('Silhouette Score: ', silhouette_score(X_pca, model_kmeans_pca.labels_))
print('Calinski harabasz score: ', calinski_harabasz_score(X_pca, model_kmeans_pca.labels_))

"""**Estos resultados indican que el modelo de K-Means ha logrado formar clusters relativamente compactos y bien definidos en el conjunto de datos, mejor en comparación con Dataset Escalado.**"""

# Predicción de los clusters
Cluster_2 = model_kmeans.fit_predict(X_pca)

BDD_final['Cluster_2KM_DR']= Cluster_2

# A partir de ahora, sólo trabajaremos con "BDD_final"
BDD_final.head(3)

# 3d scatterplot using matplotlib

fig = plt.figure(figsize = (15,15))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X_pca[Cluster_2 == 0,0], X_pca[Cluster_2 == 0,0], X_pca[Cluster_2 == 0,0], s=40, color='blue', label="cluster 0")
ax.scatter(X_pca[Cluster_2 == 1,0], X_pca[Cluster_2 == 1,0], X_pca[Cluster_2 == 1,0], s=40, color='orange', label="cluster 1")
ax.scatter(X_pca[Cluster_2 == 2,0], X_pca[Cluster_2 == 2,0], X_pca[Cluster_2 == 2,0], s=40, color='green', label="cluster 2")
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_zlabel('PC3')
ax.legend()
plt.show()

"""#Clustering Algorithm: DBSCAN

##Dataset Original
"""

from sklearn.neighbors import NearestNeighbors

# --- Selección de hiperparámetros eps ---

# --- Gráfica de distancias K --
neighbors = NearestNeighbors(n_neighbors = 2)
neighbors.fit(BDD2_out)

distances, *other = neighbors.kneighbors(BDD2_out)
distances = distances[:, 1]
print(distances.shape)
distances = np.sort(distances, axis = 0)
distances

plt.style.use('seaborn')
plt.figure(figsize = (8, 5))
plt.plot(distances, color = 'purple')
plt.ylabel('Distancia al K vecino más cercano')
plt.show()

# --- Selección del punto de curvatura máxima ---
kl = KneeLocator(range(len(distances)), distances, curve = 'convex', direction = 'increasing')
kl.elbow

# --- Epsilon óptimo --
epsilon = distances[kl.elbow]
epsilon

#La distancia en el punto de curvatura máxima

# --- Selección de hiperparámetros min_sample ---
silhoutte_coefs = []

for min_points in range(5, 31):
    model_d = DBSCAN(eps=epsilon, min_samples=min_points)
    model_d.fit(BDD2_out)
    silhoutte_coefs.append(silhouette_score(BDD2_out, model_d.labels_))

plt.figure(figsize=(12, 8))
plt.plot(range(5, 31), silhoutte_coefs, color='r', marker='^')
plt.xlabel('Muestras mínimas de cada cluster')
plt.ylabel('Silhoutte Score')
plt.title('Silhoutte Score - Min Samples de Clusters')
plt.show()

# --- Modelo DBSCAN ---
model_db = DBSCAN(eps = epsilon, min_samples = 30, n_jobs=-1)
model_db.fit(BDD2_out)

"""**eps: Esto significa que cualquier punto que esté a una distancia de hasta 4.242640687119285 unidades de otro punto se considera parte del mismo cluster.**

**min_muestras=30 : El parámetro min_samples especifica el número mínimo de muestras que deben estar en la vecindad de un punto para que ese punto sea considerado un "punto central". En este caso, se ha establecido que al menos 30 muestras deben estar dentro de la vecindad de un punto para que ese punto sea considerado un "punto central".**
"""

# --- Evaluación del modelo DBSCAN ---
print("DBSCAN")
print('Silhouette Score: ', silhouette_score(BDD2_out, model_db.labels_))
print('Calinski harabasz score: ', calinski_harabasz_score(BDD2_out, model_db.labels_))

"""**Se obtuvo un Silhouette Score mejor en comparación con K-means cuyo puntaje fue de 0,48 en Dataset Escalado y menor en comparación con  0.57 en Dataset reducido de K-means. Para Calinski harabasz score es muchísimo menor, ya que para K-means fue de 4858 en Dataset Escalado y 6469 en dataset reducido.**"""

# Predicción de los clusters
Cluster_3 = model_db.fit_predict(BDD2_out)

BDD_final['Cluster_3DB_DO']= Cluster_3

# A partir de ahora, sólo trabajaremos con "df_final"
BDD_final.head(3)

BDD_final['Cluster_3DB_DO'].value_counts()

"""**Se observa que hay dos etiquetas o clases ( 0 y -1). DBSCAN está llevando todos los datos a la clase cero y los valores de ruido y/o atípicos a la clase -1. Esto puede explicar el resultado bajo de Calinski harabasz score, al tener solo un cluster comparado con K-means donde teníamos 3, por lo tanto no puede compararse tanto el resultado entre algoritmos con respecto a Calinski harabasz score, sino solamente con el índice de silueta que resultó mejor para K-means**. **Hasta aquí podría decirse que DBSCAN no es tan útil para nuestra base de datos. **Sin embargo, vamos a utilizar otros valores de epsilon más bajos para verificar si se forman otros tipos de clusters.**

##Dataset reducido
"""

# --- Selección de hiperparámetros eps ---

# --- Gráfica de distancias K --
neighbors = NearestNeighbors(n_neighbors = 2)
neighbors.fit(X_pca)

distances, *other = neighbors.kneighbors(X_pca)
distances = distances[:, 1]
print(distances.shape)
distances = np.sort(distances, axis = 0)
distances

plt.style.use('seaborn')
plt.figure(figsize = (8, 5))
plt.plot(distances, color = 'red')
plt.ylabel('Distancia al K vecino más cercano')
plt.show()

# --- Selección del punto de curvatura máxima ---
kl = KneeLocator(range(len(distances)), distances, curve = 'convex', direction = 'increasing')
kl.elbow

# --- Epsilon óptimo --
epsilon = distances[kl.elbow]
epsilon

"""**Se observa una gráfica similar a la inicial y el valor de epsilon es igual**
Un valor epsilonmucho más grande implica que el algoritmo DBSCAN considera puntos como cercanos incluso si están a una distancia considerable entre sí. Esto puede llevar a la formación de clusters más grandes y menos densos, ya que puntos que están más separados aún pueden ser considerados parte del mismo cluster.
"""

# --- Selección de hiperparámetros min_sample ---
silhoutte_coefs = []

for min_points in range(5, 31):
    model_d = DBSCAN(eps = epsilon, min_samples = min_points)
    model_d.fit(X_pca)
    silhoutte_coefs.append(silhouette_score(X_pca, model_d.labels_))

plt.figure(figsize = (12, 8))
plt.plot(range(5, 31), silhoutte_coefs, color='r', marker='^')
plt.xlabel('Muestras mínimas de cada cluster')
plt.ylabel('Silhoutte Score')
plt.title('Silhoutte Score - Min Samples de Clusters')
plt.show()

"""**Aquí se observa que es indiferente la cantidad de muestras que oscilan entre 8 y 30**"""

# --- Modelo DBSCAN --- Aplicado esta vez para 8 muestras
model_db_pca = DBSCAN(eps = 0.5, min_samples = 8)
model_db_pca.fit(X_pca)

# --- Evaluación del modelo DBSCAN ---
print("DBSCAN")
print('Silhouette Score: ', silhouette_score(X_pca, model_db_pca.labels_))
print('Calinski harabasz score: ', calinski_harabasz_score(X_pca, model_db_pca.labels_))

"""**Se observa el mejor de los puntajes en cuanto a silueta, el Calinski harabasz score es bajo.**En este caso, un valor de 0,84 indica que los clusters están bastante bien definidos y separados**"""

# Predicción de los clusters
Cluster_4 = model_db_pca.fit_predict(X_pca)

BDD_final['Cluster_4DB_DR']= Cluster_4

# A partir de ahora, sólo trabajaremos con "df_final"
BDD_final.head(3)

BDD_final['Cluster_4DB_DR'].value_counts()

# 3d scatterplot using matplotlib

fig = plt.figure(figsize=(15, 15))
ax = fig.add_subplot(111)
ax.scatter(X_pca[Cluster_4 == 0], [0]*len(X_pca[Cluster_4 == 0]), s=40, color='purple', label="cluster 0")
ax.scatter(X_pca[Cluster_4 == -1], [0]*len(X_pca[Cluster_4 == -1]), s=40, color='red', label="outliers")
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.legend()
plt.show()

"""**Al observar gráficamente y ver el número de clústers formados, puede verse que el algoritmo no es útil, ya que generó 69 clases, lo cual no es efectivo para el agrupamiento de la base de datos y por lo tanto, se concluye que este algoritmo no es óptimo para nuestra base de datos**

#Clustering Algorithm: Hierarchical
"""

# Separación de caracteristicas
X = BDD_final.iloc[:,1:10]
print(X.shape)
X.head()

#Uso del dendrograma para encontrar el número óptimo de cluster

from scipy.cluster.hierarchy import dendrogram, linkage

# Función de vinculación para agrupar datos según similud
Z = linkage(X, method='ward', metric='euclidean')
Z[:5]

# Dibujo del dendograma
labelList = list(BDD_final['Probable manera de muerte'])

plt.figure(figsize=(13, 12))
dendrogram(
    Z,
    orientation='right',
    labels=labelList,
    distance_sort='descending',
    show_leaf_counts=False,
    leaf_font_size=16
)
plt.show()

# Calcular la matriz de enlace
from scipy.cluster.hierarchy import linkage, fcluster

Z = linkage(BDD_final, method='average')

# Determinar el número de clusters usando un criterio (e.g., distancia de corte)
num_clusters = 3
Cluster_5 = fcluster(Z, num_clusters, criterion='maxclust')

# Agrego el cluster a la base
BDD_final['Cluster_5H']= Cluster_5

#Verificamos la base
BDD_final.head(3)

silhouette = silhouette_score(BDD_final, Cluster_5)
calinski_harabasz = calinski_harabasz_score(BDD2_out, Cluster_5)
print(f'Silhouette Score: {silhouette}')
print(f'Calinski-Harabasz Score: {calinski_harabasz}')

"""**De acuerdo con estos puntajes, este algoritmo tampoco es óptimo para nuestra base de datos y se concluye que K-means es el que mejor se adapta en el dataset reducido**"""
